{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d19ac06",
   "metadata": {},
   "source": [
    "# Baseline model: Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b525378",
   "metadata": {},
   "source": [
    "### 1. Prepare data set as DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f219168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "Original train_df shape:     (4209, 366)\n",
      "Transformed train_df shape:  (4209, 553)\n",
      "\n",
      " Dropped Categorical Columns\n",
      "['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8']\n",
      "\n",
      " New OHE Columns Names\n",
      "['X0_a' 'X0_aa' 'X0_ab' 'X0_ac' 'X0_ad' 'X0_af' 'X0_ai' 'X0_aj' 'X0_ak'\n",
      " 'X0_al' 'X0_am' 'X0_ao' 'X0_ap' 'X0_aq' 'X0_as' 'X0_at' 'X0_au' 'X0_aw'\n",
      " 'X0_ax' 'X0_ay']\n",
      "Total new OHE columns: 195\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df  = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# Drop columns with no variance (from EDA)\n",
    "drop_cols = ['X11','X93','X107','X233','X235','X268','X289','X290','X293','X297','X330','X347']\n",
    "train_df = train_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "test_df  = test_df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "# 3) One-Hot Encode categorical columns\n",
    "categorical_cols = ['X0','X1','X2','X3','X4','X5','X6','X8']\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False).set_output(transform='pandas')\n",
    "\n",
    "# Ohe on train categoricals\n",
    "ohe_train = ohe.fit_transform(train_df[categorical_cols])\n",
    "# Ohe on test categoricals\n",
    "ohe_test  = ohe.transform(test_df[categorical_cols])\n",
    "\n",
    "# Transformed dataframes (drop original categoricals, join OHE)\n",
    "train_transformed = train_df.drop(columns=categorical_cols).join(ohe_train)\n",
    "test_transformed  = test_df.drop(columns=categorical_cols).join(ohe_test)\n",
    "\n",
    "# Quick checks\n",
    "print(\"Shapes\")\n",
    "print(\"Original train_df shape:    \", train_df.shape)\n",
    "print(\"Transformed train_df shape: \", train_transformed.shape)\n",
    "\n",
    "print(\"\\n Dropped Categorical Columns\")\n",
    "print(categorical_cols)\n",
    "\n",
    "print(\"\\n New OHE Columns Names\")\n",
    "print(ohe.get_feature_names_out(categorical_cols)[:20])  # show first 20 OHE columns\n",
    "print(f\"Total new OHE columns: {len(ohe.get_feature_names_out(categorical_cols))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa62d36",
   "metadata": {},
   "source": [
    "### 2. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a318cb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/valid shapes -> X_train: (3367, 551)  X_valid: (842, 551)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Define X and y \n",
    "feature_cols = [c for c in train_transformed.columns if c not in ['ID', 'y']]\n",
    "X = train_transformed[feature_cols]\n",
    "y = train_transformed['y']\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train/valid shapes ->\",\n",
    "      \"X_train:\", X_train.shape, \" X_valid:\", X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee3896",
   "metadata": {},
   "source": [
    "### 3. Baseline desicion tree (no tuning)\n",
    " - No scaling needed for trees\n",
    "- This is our simplest non-linear baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f872b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Regressor — Baseline\n",
      "RMSE: 12.478\n",
      "R²:   -0.000\n",
      "\n",
      "Top 15 Feature Importances:\n",
      "X314    0.365371\n",
      "X315    0.077554\n",
      "X263    0.066144\n",
      "X119    0.057383\n",
      "X156    0.032620\n",
      "X8_t    0.027922\n",
      "X6_i    0.007898\n",
      "X6_d    0.007876\n",
      "X5_n    0.007052\n",
      "X5_q    0.006888\n",
      "X47     0.006093\n",
      "X5_r    0.006022\n",
      "X6_g    0.005870\n",
      "X136    0.005816\n",
      "X231    0.005567\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=42)  # defaults: can overfit; we'll see baseline\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "#Evaluation (RMSE, R^2) on the validation split\n",
    "\n",
    "y_pred = tree.predict(X_valid)\n",
    "rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
    "r2   = r2_score(y_valid, y_pred)\n",
    "\n",
    "print(\"\\nDecision Tree Regressor — Baseline\")\n",
    "print(f\"RMSE: {rmse:.3f}\")  # average error in seconds\n",
    "print(f\"R²:   {r2:.3f}\")    # variance explained\n",
    "\n",
    "# feature importance (optional)\n",
    "importances = pd.Series(tree.feature_importances_, index=feature_cols)\n",
    "top_imp = importances.sort_values(ascending=False).head(15)\n",
    "print(\"\\nTop 15 Feature Importances:\")\n",
    "print(top_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc085e",
   "metadata": {},
   "source": [
    "## Conclusions Decision Tree regressor NO tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae7d690",
   "metadata": {},
   "source": [
    "**RMSE: 12.478** On average, predictions are off by ~12.5 seconds.\n",
    "-This is worse than baselines with Linear Regression (Top-3 RMSE ≈ 9.6, Top-10 RMSE ≈ 8.3, Ridge RMSE ≈ 8.2)\n",
    "\n",
    "**R²: -0.000** Negative R² means the model performed no better than just predicting the mean of y. Very possible overfiting.\n",
    "\n",
    "**Feature Importances** The most useful splits came from: \n",
    "- X314 (36%),\n",
    "-  X315 (7.8%),\n",
    "- X263 (6.6%),\n",
    "- X119 (5.7%),\n",
    "- X156 (3.3%),\n",
    "- some categorical OHE features (X8_t, X6_i, X5_n, …).\n",
    "\n",
    "- These features are in line with results of correlation analysis — ex = X314, X263, X136...\n",
    "- The modle is choosing X314 (dominates)\n",
    "\n",
    "Conclusions of baseline tree: \n",
    "- This is expected: a single tree overfits by default\n",
    "- Its poor generalization (R² ≈ 0) confirms why trees are rarely used alone in practice\n",
    "- The feature importance is still useful — it highlights strong signals in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d33e7d",
   "metadata": {},
   "source": [
    "## 4. Tuning Decision tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7364cd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Regressor — Tuned\n",
      "RMSE: 8.612\n",
      "R²:   0.523\n",
      "\n",
      "Top 15 Feature Importances (Tuned Tree):\n",
      "X314    0.576160\n",
      "X315    0.122297\n",
      "X263    0.104304\n",
      "X118    0.089910\n",
      "X115    0.010380\n",
      "X142    0.009786\n",
      "X47     0.008408\n",
      "X354    0.007173\n",
      "X2_s    0.005500\n",
      "X85     0.004968\n",
      "X3_c    0.002577\n",
      "X267    0.002422\n",
      "X349    0.002409\n",
      "X152    0.002372\n",
      "X179    0.002344\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create a regularized tree\n",
    "tree_tuned = DecisionTreeRegressor(\n",
    "    random_state=42,\n",
    "    max_depth=10,        # limit tree depth to prevent overfitting\n",
    "    min_samples_split=10, # require at least 10 samples to split a node\n",
    "    min_samples_leaf=5    # each leaf must have at least 5 samples\n",
    ")\n",
    "\n",
    "# Fit tuned model\n",
    "tree_tuned.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation split\n",
    "y_pred_tuned = tree_tuned.predict(X_valid)\n",
    "\n",
    "# Evaluate\n",
    "rmse_tuned = np.sqrt(mean_squared_error(y_valid, y_pred_tuned))\n",
    "r2_tuned   = r2_score(y_valid, y_pred_tuned)\n",
    "\n",
    "print(\"\\nDecision Tree Regressor — Tuned\")\n",
    "print(f\"RMSE: {rmse_tuned:.3f}\")\n",
    "print(f\"R²:   {r2_tuned:.3f}\")\n",
    "\n",
    "# Feature importance (optional)\n",
    "importances_tuned = pd.Series(tree_tuned.feature_importances_, index=feature_cols)\n",
    "top_imp_tuned = importances_tuned.sort_values(ascending=False).head(15)\n",
    "print(\"\\nTop 15 Feature Importances (Tuned Tree):\")\n",
    "print(top_imp_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a317e7",
   "metadata": {},
   "source": [
    "## Conclusions after Tuning \n",
    "\n",
    "**RMSE: 8.612**  improvement from the untuned baseline (12.478).\n",
    "\n",
    "- Now predictions are, on average, only ~8.6 seconds off.\n",
    "- This is in the same Top-10 Linear Regression (8.3) and Ridge (8.2).\n",
    "- Tuning fixed the overfitting issue.\n",
    "\n",
    "**R²: 0.523**\n",
    "- Now the tree explains ~52% of the variance in y. Much better than the baseline (≈ 0)\n",
    "- Comparable to Top-10 Linear Regression (0.557) and Ridge (0.566)\n",
    "**Tuned tree is competitive with our best linear model (Ridge)**\n",
    "\n",
    "**Feature Importances**\n",
    "\n",
    "- X314 (57%), X315 (12%), X263 (10%), X118 (9%) dominate (confirms EDA)\n",
    "- Compared to the untuned tree (X314 = 36%) going more to stronger predictors \n",
    "- several low importances (0.2–0.5%) tree is ingnoring noise\n",
    "\n",
    "**Conclusions**\n",
    "- A single, tuned tree is already a strong non-linear competitor to Ridge model (linear model)\n",
    "- The tuned tree has similar RMSE/R² as Ridge, but with a different modeling approach\n",
    "- Radom forrest is necesary to have more control over performance (many tunned trees).trees are sensitive to changes in slplit/seed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4592f2e2",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1085bd8d",
   "metadata": {},
   "source": [
    "## Basline Model for random forrest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49faabc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Regressor — Baseline\n",
      "RMSE: 9.121\n",
      "R²:   0.465\n",
      "\n",
      "Top 15 Feature Importances (Random Forest):\n",
      "X314     0.362296\n",
      "X315     0.059009\n",
      "X118     0.030221\n",
      "X119     0.028045\n",
      "X5_ag    0.026990\n",
      "X263     0.024343\n",
      "X8_t     0.015861\n",
      "X136     0.015160\n",
      "X127     0.014599\n",
      "X5_q     0.008418\n",
      "X6_g     0.007754\n",
      "X29      0.006955\n",
      "X5_ab    0.006449\n",
      "X279     0.006249\n",
      "X47      0.006170\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create model \n",
    "# - n_estimators: number of trees (100 is default, 200 stil fast and more stable)\n",
    "# - n_jobs=-1: use all CPU cores ?\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on testing data (validation split)\n",
    "y_pred_rf = rf.predict(X_valid)\n",
    "\n",
    "# Evaluate\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_valid, y_pred_rf))\n",
    "r2_rf   = r2_score(y_valid, y_pred_rf)\n",
    "\n",
    "print(\"\\nRandom Forest Regressor — Baseline\")\n",
    "print(f\"RMSE: {rmse_rf:.3f}\")\n",
    "print(f\"R²:   {r2_rf:.3f}\")\n",
    "\n",
    "# (Optional) Feature importance — average across trees\n",
    "importances_rf = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "print(\"\\nTop 15 Feature Importances (Random Forest):\")\n",
    "print(importances_rf.sort_values(ascending=False).head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaeac6a",
   "metadata": {},
   "source": [
    "## Conclusions of baseline model for random forrest\n",
    "\n",
    "**RMSE: 9.121**  Predictions are off by about 9.1 seconds on average\n",
    "- Better than untuned tree (12.5), but worse than a tuned single tree (8.6), Ridge (8.2), and Top-10 Linear (8.3)\n",
    "- This baseline is not tuned so is overfiting (no depth limits, no leaf constraints) \n",
    "\n",
    "**R²: 0.465** The model explains ~46.5% of the variance\n",
    "- Worse than tuned single tree (52%) and Ridge (57%)\n",
    "\n",
    "**Feature Importances**\n",
    "\n",
    "**X314 (36%)** ominating. onsistent across all models.\n",
    "\n",
    "**Secondary features:** X315, X118, X119, X263, X136 — Also in correlation & tuned tree importances\n",
    "\n",
    "**New Important features** OHE features like X5_ag, X8_t Shows the forest is considering more variables than regression\n",
    "\n",
    "**Conclusion**\n",
    "- The baseline forest is better than untuned single tree, but not better than the tuned tree or Ridge\n",
    "- Why: Random forests need regularization (max_depth, min_samples_leaf, max_features) to stabilize performance\n",
    "- Default RF settings can overfit (no generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff440df",
   "metadata": {},
   "source": [
    "## Tuned Random Forest \n",
    "We add hyperparameter tuninig to control overfiting \n",
    "\n",
    "- max_depth=10: controls complexity\n",
    "- min_samples_split=10 & min_samples_leaf=5: prevents small leaves that catch noise\n",
    "- max_features='sqrt': classic RF setting? forces trees to see different subsets of features: better generalization\n",
    "- n_estimators=200: enough trees for stability without being too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cd5c8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features=sqrt  | RMSE: 8.167 | R²: 0.571\n",
      "max_features=0.5   | RMSE: 8.056 | R²: 0.583\n",
      "max_features=0.3   | RMSE: 8.057 | R²: 0.583\n"
     ]
    }
   ],
   "source": [
    "for mf in ['sqrt', 0.5, 0.3]:\n",
    "    rf_try = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=12,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        max_features=mf,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_try.fit(X_train, y_train)\n",
    "    preds = rf_try.predict(X_valid)\n",
    "    rmse = np.sqrt(mean_squared_error(y_valid, preds))\n",
    "    r2   = r2_score(y_valid, preds)\n",
    "    print(f\"max_features={mf:<5} | RMSE: {rmse:.3f} | R²: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77027a8f",
   "metadata": {},
   "source": [
    "## Conclusions Tuned Random Forest\n",
    "** Baseline Random Forest**\n",
    "- RMSE ≈ 9.12, R² ≈ 0.465: Worse than Ridge and tuned single tree.\n",
    "\n",
    "**Tuned Random Forest**\n",
    "- sqrt → RMSE 8.167, R² 0.571\n",
    "\n",
    "- 0.5 → RMSE 8.056, R² 0.583\n",
    "\n",
    "- 0.3 → RMSE 8.057, R² 0.583\n",
    "\n",
    "**Conclusions**\n",
    "\n",
    "- After tuning, the Random Forest is better than all linear models (Top-10 Regression, Ridge) and than a tuned single tree\n",
    "\n",
    "- **RMSE ~ 8.05 (best so far), R² ~ 0.58 (highest variance explained)** \n",
    "\n",
    "\n",
    "# TUNED RANDOM FORREST IS THE BEST MODEL FOR THE MERCEDES DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2561d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fdf0f8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ede6eb3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b06c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc2576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b582811e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
